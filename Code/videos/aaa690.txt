%%%%
%
% This is the documentclass for the preparation of
% manuscripts for Revista Mexicana de Física,
% Revista Mexicana de Física E and 
% Suplemento de la Revista Mexicana de Física
%
%%%%%
\documentclass{rmf-d}
\usepackage{nopageno,rmfbib,multicol,times,epsf,amsmath,amssymb,cite}
\usepackage[T1]{fontenc} %Especial para español (for spanish)
%\usepackage[spanish, mexico]{babel}
\usepackage[]{caption2}
\usepackage{graphicx}
\usepackage{blindtext}
\usepackage{color}
\usepackage{hyperref}
\usepackage{minted}


\def\rmfcornisa{ENPM690 Robotics Learning \hfill{\bf Final Project Report} 
}
\newcommand{\ssc}{\scriptscriptstyle}
%
\def\rmfcintilla{ Final Project Report - ENPM690  }
\clearpage \rmfcaptionstyle \pagestyle{myheadings}
\setcounter{page}{1}

\begin{document}


%%%%%
%
% Please provide the following information
%
%%%%%
\title{ Autonomous Navigation of Car in Highway using Deep-Q Learning.
\vspace{-6pt}}
\author{Mothish Raj Venkatesan Kumararaj}
\address{Engineering Department, University of Maryland, College Park, Prince George County, mr2997@umd.edu}
%%%%%
%
% Use as many authors and addresses as required
%
%%%%%
\author{ }
\address{ }
\author{ }
\address{ }
\author{ }
\address{ }
\author{ }
\address{ }
\author{ }
\address{ }
\maketitle
%%%%%
%
% To be filled by the Editorial Team of RMF, RMF-E 
% and SRMF
%
%%%%%
% \recibido{15 April 2022}{16 April 2023
% \vspace{-12pt}}
\vspace{-35pt}
\begin{abstract}
\vspace{1em} 

\textbf{Abstract: In this project, I try to address a problem statement of navigation of cars in highway with dynamic traffic test cases, I used Deep-Q learning method  which is a type of reinforcement learning and designed my own model to navigate in a volatile environment. Additionally, we use another environment for the vehicle to park in a parking lot simulation which uses a model based learning method.}
\\ \bf{\textbf{\textit{Key Words: Deep-Q Learing, Model-Based Learing, Tensorflow, Pytorch, Neural Network}}}
\end{abstract}

\begin{multicols}{2}


\section{Introduction}
Autonomous navigation has been a topic of the decade. In the current scenario, most of the automobile industries trying to achieve the autonomous navigation for most of their models. Most of the cars runs with the PID controller loop with obstacle avoidance. 

In the project, I try to implement reinforcement learning techniques to improve the performance of the navigation system. For the project, I use Deep-Q learning method which is a type of reinforcement learning that is a combination of Neural network learning and Q-learning. In place of the Q-table we replace it with neural network to predict the possible action for the next state and the simulation is carried out for the dynamic environment and results are plotted. 

Additionally, I also implemented a model-based learning technique for the parking lot system since the parking environment is not subjective to a drastic change as a highway environment. For the scope of the project for the parking simulation, I concentrated on the orientation of parking and the reaching the goal state. Simulation was also carried out for the parking lot and results are plotted.


\section{Related Works}
\subsection{Deep Q learning}
 Q-learning is a reinforcement learning algorithm which learn the value of the action in a particular state and it does not require any model of the environment hence its a model free learning. Since the Q-learning technique traverse through the Q-table, if the size of the state space increases the iteration will be time consuming to traverse through the table. Hence we introduce the Deep Q-learning where the Q-table is replaced with a neural network model. 
 
 The input is provided to the neural network that contains hidden layers and input layers according to the user choice the output is used to take the next course of action in the Deep Q learning model. 

\begin{figure}[H]
 \includegraphics[width=\linewidth]
    {dqn}
 \caption{Deep-Q Learning Model}
 \label{figDQN}
\end{figure}
\noindent


\begin{figure}[H]
 \includegraphics[width=\linewidth]
    {nn}
 \caption{Neural Network Model}
 \label{fig:NN}
\end{figure}
\noindent
\subsection{Model Based learning}
 Model based learning is an approach where the  the information or assumption about the problem domain are made explicit in the form of a model. This model is then used to create a model-specific to learn or reason about the domain.
 
 The reason for introduction of model based learning is because in model-free learning the model runs with reward as the reinforcement goal but it can render an ineffective output where the environment has only sparse rewards where the chance of getting a rewards in a random pattern is almost negligible. 
 
 
\begin{figure}[H]
 \includegraphics[width=\linewidth]
    {model}
 \caption{Model Based Learning}
 \label{figDQN}
\end{figure}
\noindent

\subsection{LTI}
 linear time-invariant system (LTI) can be defined as a system that produce output signal from any input which  is subjected to the constraints of linearity and time-invariance. linearity  can be defined is a mathematical model of a system based on the use of a linear operator and Time-invariance can be defined as the system system function that is not a direct function of time. The general equation is given by 
 \begin{equation}
  f_{\theta}(x,u) = A_{\theta}(x,u)x + B_{\theta}(x,u)u
  \end{equation}
\section{Methodology}
\subsection{Highway Navigation}
In highway simulation, we utilize the deep Q learning method where we create our own DQN Model and test it with different type of inputs to the agent. 
For the each state input, the neural network will estimate the Q-value for each action that can be taken from the state and the objective is to approximate the Q function to satisfy the Bellman equation (1).

\begin{equation}
Q_{*} = E[R_{t+1} + \gamma \max_{a'} Q_{*}(s',a') ]
\end{equation}

Loss function is then calculated from the difference in output Q value and target Q value. From the calculated output the weights are then updated in the Neural network model using backward propagation. 

\subsubsection{Algorithm}
The algorithm we follow to train the highway environment using Deep-Q Learning is mentioned below :

\begin{enumerate}
\item Initialize replay memory.
\item Initialize the policy networks.
\item Initialize the target network similar to policy network.
\item For each episode:
\begin{enumerate}
\item Initialize the starting state.

\item For each time step:
\begin{enumerate}
\item Select an action.
\item exploration or exploitation
\item Execute selected action.
\item Observe reward and next state.
\item Store experience in replay memory.
\item Sample random batch from replay memory.
\item Preprocess states from batch.
\item Pass batch of preprocessed states to policy network.
\item Calculate loss between output Q-values and target Q-values where we pass current state to the target network for the next state.
\item Gradient descent updates weights in the policy network to minimize loss.
\item After certain time steps, weights in the target network are updated to the weights in the policy network.

\end{enumerate}
\end{enumerate}
\end{enumerate}

\subsubsection{Exploration vs Exploitation}
In exploration the agent will try to select the action whose environment is least explored and the exploitation part is where the agent prefers a action that is known and yields a higher reward. In initially we use the epsilon greedy rate $\epsilon$ to be 1 then in the later stages the agent becomes greedy and the rate decays and then switch to exploitation. 

\subsubsection{Experience Replay}
Experience replay technique is used during training of the network where we store the experience in the replay memory as the experience $e_{t}$ at time t

\begin{equation}
e_{t} = (s_{t},a_{t},r_{t+1},s_{t+1})
\end{equation}

In this data set, $s_{t}$ and $a_{t}$ is the state and the action taken on the state. $r_{t+1}$ is the reward of the state action pair $s_{t}$ and $a_{t}$. $s_{t+1}$ is the next state of the environment.

\begin{figure}[H]
\begin{center}
 \includegraphics [scale=1.5] {relay}
 \end{center}

 \caption{Experience Replay Dataset}
 \label{fig:ER}
\end{figure}
\noindent

The agent's experience is stored at each time-step is stored in the replay memory set of finite size of last experiences. We randomly sample from the replay memory for training the agent. The reason we do that is since initially we are training the agent only with sequential experience from the environment which creates consecutive samples, which can result in correlation between each data in the graph leads to inefficient training. By introducing experience replay, we can break the correlation and provide a better training to the agent. 

\subsubsection{Loss Function}
The loss function we use in our training model is the Mean Squared error. It is the  simplest and most common loss function. To calculate the Mean Squared error, you take the difference between your model's predictions and the ground truth, square it, and average it out across the whole data set. 

\subsubsection{Loss Calculation}
After all the experience data set is stored in the replay memory we then sample a random data from the replay memory then we send the batch of data into the model. After forward propagation, we receive the Q values for the state action pair. Then, we subtract it from the optimal Q value for that state and action obtained from the right hand side of Bellman's equation (1).  

The loss calculation formula is given by, 

\begin{equation}
loss = Q_{*}(s,a) - Q(s,a)
\end{equation}
\begin{equation}
loss = E[R_{t+1} + \gamma \max_{a'} Q_{*}(s',a') ] - E[\sum_{n=0}^{\infty} \gamma^n R_{t+n+1}]
\end{equation}

In the given equation, the s' and a' are the state action pair of the next time step. Hence, we use forward propagation two times in each iteration to get the Q values of the current time step and maximum Q value of the next time step. Backward propagation is the done using the loss calculated. Gradient descent is performed to update the weights in the model.

\subsubsection{Target and  Policy network}
In the above given equation for loss function, the q' and s' are the state and the Q values of the next time step. If we are using the same neural network then we need to update the weights to calculate the next state and Q values(s',q'). After updating the weights of the neural network, the Q-values move towards the Q-Target values but also Q-Target values move in the same direction which makes the optimization chasing its own tail and unstable. Hence we create a target network which is a exact copy of the policy network. Its weights are frozen  policy network's initial weights, and we update the weights in the target network taking the weights from the policy network after certain specified time step.


\subsubsection{Optimization}
For the optimization of the loss function, I used the Adaptive Moment Estimation which is a algorithm for optimization for gradient descent. It is a combination of the ‘gradient descent with momentum’ algorithm and the ‘RMSP’ algorithm,  efficient with large problem involving a lot of data or parameters.

\subsection{Parking System}
In addition to the proposed problem, I also have tried implementing a Model-Based reinforcement learning technique to simulate the parking environment. The parking system is a continuous control system and I have tried to implement Linear Time Variant system



\subsubsection{Model based learning}
In this particular environment we consider a optimal control problem of Markov Decision Process where the reward function \textbf{R} is known to us ant the deterministic dynamics \textbf{$s_{t+1}=f(s_{t},a_{t})$} is unknown


In the given environment, we learn the model of the dynamics $f_{\theta} \approx f$ using the regression on interaction  data and then we use the learn dynamic model to find the optimal trajectory

\begin{equation}
 \max_{a_{0},a_{1},a_{2}..} \sum_{t=0}^{\infty}\gamma^{t}R(s_{t},a_{t})
\end{equation}

\subsubsection{Dynamic Model}
Initially I randomly interact with the environment to collect the experience batch.
\begin{equation}
 D= {s_{R},a_{t},s_{t+1}}_{t\epsilon|1,N|}
\end{equation}
Then I designed a model to represent the dynamic system. For this project I utilized a structured model inspired from the Linear Time-Invariant systems(LTI) which can be represented as following.

\begin{equation}
 f_{\theta}(s,a) = A_{\theta}(s,a)s + B_{\theta}(s,a)a
\end{equation}
where s and a are the state and action pairs and we parameterize $A_{\theta}$ and $B_{\theta}$ as two neural network.

\subsubsection{Loss Calculation}
For the loss function for the given problem we use the Mean Squared Error, calculated by difference between your model's predictions and the ground truth, square it, and average it out across the whole data set. the MSE is given by the formula. 

\begin{equation}
 L^{2}(f_{\theta};D) = 1/|D|  \sum_{s_{t},a_{t},s_{t+1} \epsilon D} {||s_{t+1} - f_{\theta}(s_{t},a_{t})||}^{2}
\end{equation}


\subsubsection{Optimization}
For the optimization of the loss function, I used the Adaptive Moment Estimation which is a algorithm for optimization for gradient descent. For the particular problem we use stochastic gradient descent which is a  is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.


\section{Simulation}
For the simulation environment, I utilized the library called gym specially designed to train reinforcement models. For this specific project I used the highway environment and parking lot environment which can be imported using gym. 

\subsection{Highway Simulation}
The highway simulation is an dynamic environment which contains a ego vehicle which can be controlled by the user. The input commands for the ego- vehicle is discrete with the commands that are provided below. 

 0 : Switch to left lane

 1 : Idle
 
 2 : Switch to Right lane
 
 3 : Move Faster 
 
 4 : Move Slower
 
 \begin{figure}[H]
 \includegraphics[width=\linewidth]
    {env}
 \caption{Highway Environment}
 \label{figDQN}
\end{figure}
\noindent
 
 The goal of the environment is to navigate with the ego vehicle quickly in different environment. The inputs we provide to the environment are of two types which are provided below. 
 
 \subsubsection{Observation Input}
 The observation input is specific to the environment which provided in the observation stack. this input is provided to the Neural network input and the weights are altered accordingly. The input type and the environment are provided below 

\begin{figure}[H]
 \includegraphics[width=\linewidth]
    {obs}
 \caption{Kinematic Observation}
 \label{figDQN}
\end{figure}
\noindent
 
This is the type of input used in the observation input type of code. Since we are controlling only the ego vehicle, I only sent the observation of the eqo-vehicle which is the first row in the observation stack, which is a kinematic type observation that contains $["x", "y", "vx", "vy", "cos_h", "sin_h"]$ as the observation stack. 

 \subsubsection{Image Input}
 For the input image, though there is an observation stack for different type input is available in the code. I tried to implement my own input which is taken from the environment and trained the model to perform. The given input is processed and the processed image is trained in the neural network environment.
 

 \begin{figure}[H]
 \includegraphics[width=\linewidth]
    {proc}
 \caption{}
 \label{figproc}
\end{figure}
\noindent

 \begin{figure}[H]
 \includegraphics[width=\linewidth]
    {stat}
 \caption{}
 \label{stat}
\end{figure}
\noindent

\subsection{Parking Simulation}
Parking simulation is a model of parking lot where the world is not an dynamic environment. The reward point for this environment is that the robot should reach its designated parking point in the reverse orientation. The parking simulation environment is a continuous control environment where it implements a reach-type task, where the agent observes their position and speed and must control their acceleration and steering so as to reach a given goal.
 \begin{figure}[H]
 \includegraphics[width=\linewidth]
    {park}
 \caption{park Environment}
 \label{figDQN}
\end{figure}
\noindent

 \begin{figure}[H]
 \includegraphics[width=\linewidth]
    {traj}
 \caption{Car Trajectories}
 \label{stat}
\end{figure}
\noindent


\section{Results}
The performance graph are calculated with moving average and loss calculation reduction over time. the performance graph for each parameters and for the different environment is provided below

\subsection{Highway Simulation}
 \subsubsection{Observation Input}
This given moving average and the loss function below is for the observation type input of the parking lot simulation. which has 1 hidden layer and 5 output actions. The following result is trained with following parameter 


$Episode: 300$



$batch size: 256$



$Time Step: 1$



$Epsilon Decay Rate: 0.001$



$Initial Epsilon: 1$

$Optimizer: Adam$

$Loss Function: MSE $
\\
\\
For the neural network in the DQN model the following parameters are provided as input

$Input Layer: Number of State Features$

$Hidden Layer 1: 24$

$Hidden Layer 2: 32$

$Output Layer: 5$

$Activation Function: ReLu$




 \begin{figure}[H]
 \includegraphics[width=\linewidth]
    {loss_obs}
 \caption{Loss Graph Observation Input - Highway}
 \label{figDQN}
\end{figure}
\noindent

 \begin{figure}[H]
 \includegraphics[width=\linewidth]
    {result_obs}
 \caption{Moving Average Graph Observation Input - Highway}
 \label{figDQN}
\end{figure}
\noindent

\subsubsection{Observation-  Observation Input}
 From the given graph, it is clear that the performance of the observation input is not great and the loss function curve is almost flat towards the end. One of the observation that has been seen in the environment is that, since we are controlling only the ego-vehicle in our kinematic observation stack, the vehicle tends to maintain one wheel and control the speed to avoid collision instead of shifting to another lane.
 
 \subsubsection{Image Input}
This given moving average and the loss function below is for the image type input of the parking lot simulation. which has 1 hidden layer and 5 output actions. The following result is trained with following parameter  

$Episode: 300$

$batch size: 256$

$Time Step: 1$

$Epsilon Decay Rate: 0.001$

$Initial Epsilon: 1$

$Optimizer: Adam$

$Loss Function: MSE $
\\
\\
For the neural network in the DQN model the following parameters are provided as input

$Input Layer: Number of State Features$

$Hidden Layer 1: 24$

$Hidden Layer 2: 32$

$Output Layer : 5$

$Activation Function: ReLu$


 \begin{figure}[H]
 \includegraphics[width=\linewidth]
    {loss_dqn}
 \caption{Loss Graph Image Input - Highway}
 \label{lossimg}
\end{figure}
\noindent

 \begin{figure}[H]
 \includegraphics[width=\linewidth]
    {result_img}
 \caption{Moving Average Graph Image Input - Highway}
 \label{resImg}
\end{figure}
\noindent
\subsubsection{Observation-  Image Input}
 From the given graph, it is clear that the performance is better compared to the observation input type and the moving average is increased gradually over the 300 episodes. The performance of the overall system was not up to the mark because I was trying to generalize the code and did not use the observation image stack from the environment, instead I tried to create a code where a process image is generated by me. The performance can be improved by sending more than two images to the system and by training more episodes. 
 
\subsection{Parking Simulation}
For the parking simulation, I calculated the loss graph and provided the results of the loss graph for the following parameters

$Episode: 1500$

$training ratio: 0.7$

$Optimizer: ADAM$

$Loss Function: MSE$
\\

The neural network for A and B on the Linear Time Invariant (LTI) system is given as following:

$Input Layer A: State size + Action size$

$Input Layer B: State size + Action size$

$Hidden Layer 1 A: 64$

$Hidden Layer 1 B: 64$

$Output Layer A: State size * State size$

$Output Layer B: State size * Action size$

$Activation Function: ReLu$

 \begin{figure}[H]
 \includegraphics[width=\linewidth]
    {loss_park}
 \caption{Loss Model Based Learning- Parking}
 \label{resImg}
\end{figure}
\noindent
\subsubsection{Observation-  Model Based Learning}
The results provided were performing in a good rate and we filtered the best episodes using Cross entropy method. The loss graph curved and reduced gradually and provided the desired results compared to the model-free learning. 
\section{References}


\begin{thebibliography}{99}
\bibitem{ELK}Highway Environment and Parking Environment
\url{https://highway-env.readthedocs.io/en/latest/index.html}

\bibitem{Griffiths} D.J. Griffiths, 
Introduction to Electrodynamics, 
2nd ed. 
(Prentice Hall, Englewood Cliffs, NJ, 1989), 
pp. 331–334.
\end{thebibliography}







\end{multicols}

% \medline
\begin{multicols}{2}
%%%%%%%%%%%%%%%
%
%Using BibTeX
% \nocite{*}
% \bibliographystyle{rmf-style}
% \bibliography{ref}
% %
%%%%%%%%
%
%Introducing references manually
%


\end{multicols}
\end{document}
