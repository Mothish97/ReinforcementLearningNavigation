{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa958f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#!pip install highway-env\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T    \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d71cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aac2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "       \"observation\": {\n",
    "           \"type\": \"GrayscaleObservation\",\n",
    "           \"observation_shape\": (128, 64),\n",
    "           \"stack_size\": 2,\n",
    "           \"weights\": [0.2989, 0.5870, 0.1140],  # weights for RGB conversion\n",
    "           \"scaling\": 1.75,\n",
    "       },\n",
    "       \"policy_frequency\": 2\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70240682",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, height,width):\n",
    "        super().__init__()\n",
    "        self.double()\n",
    "        self.fc1 = nn.Linear(in_features=64, out_features=32)   \n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=16)\n",
    "        self.out = nn.Linear(in_features=16, out_features=5)            \n",
    "\n",
    "    def forward(self, t):\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5518617",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f52b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2beea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "        math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d7978b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            #print(\"Action Explore \", action)\n",
    "            return torch.tensor([action]).to(self.device) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #print(\"Action Exploit\",policy_net(state).unsqueeze(dim=0).argmax(dim=1).argmax().item())\n",
    "                action=policy_net(state[-1]).unsqueeze(dim=0).argmax(dim=1).argmax().item()\n",
    "                return torch.tensor([action]).to(self.device) # exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b588b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class highwayModel():\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.env = gym.make('highway-v0').unwrapped\n",
    "        self.env.configure(config)\n",
    "        self.env.reset()\n",
    "        self.done = False\n",
    "        self.current_state = None\n",
    "    def reset(self):\n",
    "        self.current_state = self.env.reset()\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "    \n",
    "    def num_actions_available(self):\n",
    "        return self.env.action_space.n        \n",
    "    \n",
    "    def take_action(self, action): \n",
    "        #numpy= action.numpy()\n",
    "        #print(\"Type:  \",numpy.ndim, \"Val :\", numpy)\n",
    "#         if(numpy.ndim>1):\n",
    "#             self.current_state, reward, self.done, _ = self.env.step(numpy[0][0])\n",
    "#         else:\n",
    "        self.current_state, reward, self.done, _ = self.env.step(action.item())\n",
    "        return torch.tensor([reward], device=self.device)\n",
    "    \n",
    "    def get_state(self):\n",
    "        if self.done:\n",
    "            #print(\"done sat\")\n",
    "            return torch.zeros_like(torch.tensor(self.current_state[0]), device=self.device).float()\n",
    "        else:\n",
    "            #print(\"non done sat\")\n",
    "            #print(torch.tensor(self.current_state[0], device=self.device).float())\n",
    "            print(self.current_state[-1].shape)\n",
    "            return torch.tensor(self.current_state[-1], device=self.device).float()\n",
    "    \n",
    "    def num_state_features(self):\n",
    "        return self.env.observation_space.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d1dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6f308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "245aa4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "    #print(\"Batch:\",batch.action )\n",
    "    t1 = torch.stack(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.stack(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3b90d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    device = torch.device(\"cpu\")\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        print(\"action Size: \",actions.unsqueeze(-1).size())\n",
    "        print(\"state Size: \",policy_net(states).size())\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    \n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states):                \n",
    "        final_state_locations = next_states.flatten(start_dim=1) \\\n",
    "            .max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "        values = torch.zeros(batch_size).to(QValues.device)\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42f0b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d4e13b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(values, moving_avg_period,loss,num_of_epi_epi):\n",
    "    plt.figure(2)\n",
    "    plt.clf()        \n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(values)\n",
    "    \n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    data=\"Moving avg:\"+ str(moving_avg[-1])\n",
    "    #print(\"Mo\",moving_avg)\n",
    "    plt.title(data)\n",
    "    plt.plot(moving_avg)    \n",
    "    plt.savefig('result.png')\n",
    "    #plt.pause(0.001)\n",
    "    \n",
    "    plt.clf() \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(num_of_epi_epi,loss)\n",
    "    #plt.pause(0.001)\n",
    "    plt.savefig('loss.png')\n",
    "    if(len(loss)>1):\n",
    "        print(\"Loss : \",loss[-1])\n",
    "    print(\"Episode\", len(values), \"\\n\", \\\n",
    "        moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
    "    if is_ipython: display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da87f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "gamma = 0.99\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10\n",
    "memory_size = 100000\n",
    "lr = 0.0000001\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81d44c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "314092d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "em = highwayModel(device)\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162c246b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7d85d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(strategy, em.num_actions_available(), device)\n",
    "memory = ReplayMemory(memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c002f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_net = DQN(em.num_state_features(),em.num_state_features()).to(device)\n",
    "# target_net = DQN(em.num_state_features(),em.num_state_features()).to(device)\n",
    "\n",
    "policy_net = DQN(128,64).to(device)\n",
    "target_net = DQN(128,64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64c05d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(em.num_state_features())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ddd8e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (out): Linear(in_features=16, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "405aac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ac5a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c986281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val=[]\n",
    "num_of_epi=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e80bf89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "(128, 64)\n",
      "tensor([4])\n",
      "(128, 64)\n",
      "tensor([3])\n",
      "(128, 64)\n",
      "tensor([3])\n",
      "(128, 64)\n",
      "tensor([4])\n",
      "(128, 64)\n",
      "tensor([4])\n",
      "(128, 64)\n",
      "tensor([0])\n",
      "(128, 64)\n",
      "tensor([4])\n",
      "(128, 64)\n",
      "tensor([0])\n",
      "(128, 64)\n",
      "tensor([3])\n",
      "(128, 64)\n",
      "tensor([0])\n",
      "(128, 64)\n",
      "action Size:  torch.Size([100, 1])\n",
      "state Size:  torch.Size([100, 128, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m states, actions, rewards, next_states \u001b[38;5;241m=\u001b[39m extract_tensors(experiences)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#print(\"States: \",states[0])\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m current_q_values \u001b[38;5;241m=\u001b[39m \u001b[43mQValues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_current\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m QValues\u001b[38;5;241m.\u001b[39mget_next(target_net, next_states)\n\u001b[0;32m     27\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m (next_q_values \u001b[38;5;241m*\u001b[39m gamma) \u001b[38;5;241m+\u001b[39m rewards\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mQValues.get_current\u001b[1;34m(policy_net, states, actions)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction Size: \u001b[39m\u001b[38;5;124m\"\u001b[39m,actions\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate Size: \u001b[39m\u001b[38;5;124m\"\u001b[39m,policy_net(states)\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    \n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    for timestep in count():\n",
    "        em.render()\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        #print(action)\n",
    "        #numpy= action.numpy()\n",
    "        #print(\"Action:  \",action)\n",
    "        #print(\"state:  \",state[0])\n",
    "        print(policy_net(state).size())\n",
    "        reward = em.take_action(action)\n",
    "        \n",
    "        next_state = em.get_state()\n",
    "        \n",
    "        memory.push(Experience(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "        \n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "            #print(\"States: \",states[0])\n",
    "            \n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "            \n",
    "            \n",
    "\n",
    "            loss = F.mse_loss(current_q_values.float(), target_q_values.unsqueeze(1).float())\n",
    "            loss_val.append(loss.item())\n",
    "            num_of_epi.append(episode)\n",
    "            #printd(loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #break\n",
    "\n",
    "        if em.done:\n",
    "            \n",
    "            episode_durations.append(timestep)\n",
    "            plot(episode_durations, 100,loss_val,num_of_epi)\n",
    "            break\n",
    "    #print(\"loss:\", loss)\n",
    "    #print(\"Num of episode: \", loss_graph)\n",
    "    \n",
    "    #break\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e7cead",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "print(em.num_state_features())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3619ba32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b8d336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
