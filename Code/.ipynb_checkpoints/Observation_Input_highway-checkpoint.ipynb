{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa958f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T    \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d71cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70240682",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_state_features):\n",
    "        super().__init__()\n",
    "         \n",
    "        self.fc1 = nn.Linear(in_features=num_state_features, out_features=24)   \n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=5)            \n",
    "\n",
    "    def forward(self, t):\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5518617",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f52b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2beea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "        math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d7978b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "\n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)\n",
    "            #print(\"Action Explore \", action)\n",
    "            return torch.tensor([action]).to(self.device) # explore      \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                #print(\"Action Exploit\",policy_net(state).unsqueeze(dim=0).argmax(dim=1).argmax().item())\n",
    "                action=policy_net(state).unsqueeze(dim=0).argmax(dim=1).argmax().item()\n",
    "                return torch.tensor([action]).to(self.device) # exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b588b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class highwayModel():\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.env = gym.make('highway-v0').unwrapped\n",
    "        self.env.reset()\n",
    "        self.done = False\n",
    "        self.current_state = None\n",
    "    def reset(self):\n",
    "        self.current_state = self.env.reset()\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "    \n",
    "    def num_actions_available(self):\n",
    "        return self.env.action_space.n        \n",
    "    \n",
    "    def take_action(self, action): \n",
    "        #numpy= action.numpy()\n",
    "        #print(\"Type:  \",numpy.ndim, \"Val :\", numpy)\n",
    "#         if(numpy.ndim>1):\n",
    "#             self.current_state, reward, self.done, _ = self.env.step(numpy[0][0])\n",
    "#         else:\n",
    "        self.current_state, reward, self.done, _ = self.env.step(action.item())\n",
    "        return torch.tensor([reward], device=self.device)\n",
    "    \n",
    "    def get_state(self):\n",
    "        if self.done:\n",
    "            \n",
    "            #print(\"done sat\")\n",
    "            return torch.zeros_like(torch.tensor(self.current_state[0]), device=self.device).float()\n",
    "        else:\n",
    "            #print(\"non done sat\")\n",
    "            #print(torch.tensor(self.current_state[0], device=self.device).float())\n",
    "            return torch.tensor(self.current_state[0], device=self.device).float()\n",
    "    \n",
    "    def num_state_features(self):\n",
    "        return self.env.observation_space.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d1dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad6f308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "245aa4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "    #print(\"Batch:\",batch.action )\n",
    "    t1 = torch.stack(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.stack(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3b90d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    device = torch.device(\"cpu\")\n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        #print(\"action Size: \",actions.unsqueeze(-1).size())\n",
    "        #print(\"state Size: \",policy_net(states).size())\n",
    "        return policy_net(states).gather(dim=1, index=actions.unsqueeze(-1))\n",
    "    \n",
    "    @staticmethod        \n",
    "    def get_next(target_net, next_states):                \n",
    "        final_state_locations = next_states.flatten(start_dim=1) \\\n",
    "            .max(dim=1)[0].eq(0).type(torch.bool)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "        values = torch.zeros(batch_size).to(QValues.device)\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42f0b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d4e13b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(values, moving_avg_period,loss,num_of_epi_epi):\n",
    "    plt.figure(2)\n",
    "    plt.clf()        \n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(values)\n",
    "    \n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    data=\"Moving avg:\"+ str(moving_avg[-1])\n",
    "    #print(\"Mo\",moving_avg)\n",
    "    plt.title(data)\n",
    "    plt.plot(moving_avg)    \n",
    "    plt.savefig('result.png')\n",
    "    #plt.pause(0.001)\n",
    "    \n",
    "    plt.clf() \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(num_of_epi_epi,loss)\n",
    "    #plt.pause(0.001)\n",
    "    plt.savefig('loss.png')\n",
    "    if(len(loss)>1):\n",
    "        print(\"Loss : \",loss[-1])\n",
    "    print(\"Episode\", len(values), \"\\n\", \\\n",
    "        moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
    "    if is_ipython: display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da87f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "gamma = 0.99\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "target_update = 10\n",
    "memory_size = 100000\n",
    "lr = 0.0000001\n",
    "num_episodes = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81d44c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "314092d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "em = highwayModel(device)\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162c246b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7d85d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(strategy, em.num_actions_available(), device)\n",
    "memory = ReplayMemory(memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c002f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_net = DQN(em.num_state_features(),em.num_state_features()).to(device)\n",
    "# target_net = DQN(em.num_state_features(),em.num_state_features()).to(device)\n",
    "\n",
    "policy_net = DQN(em.num_state_features()).to(device)\n",
    "target_net = DQN(em.num_state_features()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c05d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ddd8e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (fc1): Linear(in_features=5, out_features=24, bias=True)\n",
       "  (fc2): Linear(in_features=24, out_features=32, bias=True)\n",
       "  (out): Linear(in_features=32, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "405aac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ac5a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c986281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val=[]\n",
    "num_of_epi=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e80bf89a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state, policy_net)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#numpy= action.numpy()\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#print(\"Action:  \",action)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#print(\"state:  \",state[0])\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#print(policy_net(state).size())\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[43mem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m next_state \u001b[38;5;241m=\u001b[39m em\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[0;32m     16\u001b[0m memory\u001b[38;5;241m.\u001b[39mpush(Experience(state, action, next_state, reward))\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mhighwayModel.take_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, action): \n\u001b[0;32m     20\u001b[0m         \u001b[38;5;66;03m#numpy= action.numpy()\u001b[39;00m\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;66;03m#print(\"Type:  \",numpy.ndim, \"Val :\", numpy)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#         if(numpy.ndim>1):\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#             self.current_state, reward, self.done, _ = self.env.step(numpy[0][0])\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#         else:\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_state, reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([reward], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\highway_env\\envs\\common\\abstract.py:216\u001b[0m, in \u001b[0;36mAbstractEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_type\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m    219\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward(action)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001b[0m, in \u001b[0;36mAbstractEnv._simulate\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \\\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_control\u001b[39m\u001b[38;5;124m\"\u001b[39m] \\\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_type\u001b[38;5;241m.\u001b[39mact(action)\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimulation_frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\highway_env\\road\\road.py:324\u001b[0m, in \u001b[0;36mRoad.act\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;124;03m\"\"\"Decide the actions of each entity on the road.\"\"\"\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvehicles:\n\u001b[1;32m--> 324\u001b[0m     \u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\highway_env\\vehicle\\behavior.py:101\u001b[0m, in \u001b[0;36mIDMVehicle.act\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Longitudinal: IDM\u001b[39;00m\n\u001b[0;32m    100\u001b[0m front_vehicle, rear_vehicle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroad\u001b[38;5;241m.\u001b[39mneighbour_vehicles(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index)\n\u001b[1;32m--> 101\u001b[0m action[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macceleration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macceleration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mego_vehicle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mfront_vehicle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfront_vehicle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mrear_vehicle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrear_vehicle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# When changing lane, check both current and target lanes\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane_index \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_lane_index:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\highway_env\\vehicle\\behavior.py:151\u001b[0m, in \u001b[0;36mIDMVehicle.acceleration\u001b[1;34m(self, ego_vehicle, front_vehicle, rear_vehicle)\u001b[0m\n\u001b[0;32m    147\u001b[0m acceleration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCOMFORT_ACC_MAX \u001b[38;5;241m*\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(\u001b[38;5;28mmax\u001b[39m(ego_vehicle\u001b[38;5;241m.\u001b[39mspeed, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m/\u001b[39m ego_target_speed, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDELTA))\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m front_vehicle:\n\u001b[1;32m--> 151\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[43mego_vehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlane_distance_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfront_vehicle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m     acceleration \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCOMFORT_ACC_MAX \u001b[38;5;241m*\u001b[39m \\\n\u001b[0;32m    153\u001b[0m         np\u001b[38;5;241m.\u001b[39mpower(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesired_gap(ego_vehicle, front_vehicle) \u001b[38;5;241m/\u001b[39m utils\u001b[38;5;241m.\u001b[39mnot_zero(d), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m acceleration\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\highway_env\\vehicle\\objects.py:164\u001b[0m, in \u001b[0;36mRoadObject.lane_distance_to\u001b[1;34m(self, other, lane)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lane:\n\u001b[0;32m    163\u001b[0m     lane \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlane\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lane\u001b[38;5;241m.\u001b[39mlocal_coordinates(other\u001b[38;5;241m.\u001b[39mposition)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[43mlane\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\highway_env\\road\\lane.py:186\u001b[0m, in \u001b[0;36mStraightLane.local_coordinates\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m    184\u001b[0m delta \u001b[38;5;241m=\u001b[39m position \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m    185\u001b[0m longitudinal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdirection)\n\u001b[1;32m--> 186\u001b[0m lateral \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    \n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    for timestep in count():\n",
    "        em.render()\n",
    "        action = agent.select_action(state, policy_net)\n",
    "        #numpy= action.numpy()\n",
    "        #print(\"Action:  \",action)\n",
    "        #print(\"state:  \",state[0])\n",
    "        #print(policy_net(state).size())\n",
    "        reward = em.take_action(action)\n",
    "        \n",
    "        next_state = em.get_state()\n",
    "        \n",
    "        memory.push(Experience(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "        if memory.can_provide_sample(batch_size):\n",
    "        \n",
    "            experiences = memory.sample(batch_size)\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)\n",
    "            #print(\"States: \",states[0])\n",
    "            \n",
    "            current_q_values = QValues.get_current(policy_net, states, actions)\n",
    "            next_q_values = QValues.get_next(target_net, next_states)\n",
    "            target_q_values = (next_q_values * gamma) + rewards\n",
    "            \n",
    "            \n",
    "\n",
    "            loss = F.mse_loss(current_q_values.float(), target_q_values.unsqueeze(1).float())\n",
    "            loss_val.append(loss.item())\n",
    "            num_of_epi.append(episode)\n",
    "            #printd(loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #break\n",
    "\n",
    "        if em.done:\n",
    "            \n",
    "            episode_durations.append(timestep)\n",
    "            plot(episode_durations, 100,loss_val,num_of_epi)\n",
    "            break\n",
    "    #print(\"loss:\", loss)\n",
    "    #print(\"Num of episode: \", loss_graph)\n",
    "    \n",
    "    #break\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e7cead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3619ba32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
